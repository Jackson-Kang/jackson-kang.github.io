<html>
  <head>
    <meta charset="UTF-8">
    <title>UniSpeech MOS test</title>
  </head>
  <body>
    <article>
      <header>
        <h1>UniSpeech Mean Opinion Score test</h1>
      </header>
    </article>

    <div>
        <h3>Authors</h3>
        Minsu Kang, Sungjae Kim and Injung Kim (Handong Global University)
    </div>
    <div>
        <h3>Abstract</h3>
            We propose a novel high-fidelity expressive speech synthesis model, UniSpeech, that learns and controls multiple non-hierarchically correlated attributes without conflict. UniSpeech represents phonemes and non-linguistic attributes in a single unified embedding space. The proposed method is particularly effective in reflecting both speaker ID and emotion because it does not add the variance by the two overlapping attributes redundantly, and predicts prosodic attributes based on the speaker and emotion IDs. UniSpeech learns the unified embedding space leveraging a residual network that extends FastSpeech2. We additionally applied a data augmentation technique to improve the fidelity and controllability over the non-linguistic attributes. In experiments, the visualization results exhibited that UniSpeech successfully learned multiple attributes in the unified embedding space. As well, UniSpeech synthesized high-fidelity speech signals while controlling multiple attributes, and transferred speech style from the reference speech.

    <div>
	<br>
	<br>

	<h3> This page is under construction. The demo samples synthesized by UniSpeech will be presented soon. </h3>

    </div>
  </body>
</html>
